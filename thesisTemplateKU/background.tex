\chapter{Background and Related Works}
\label{sec:back}

Human ears start to develop between fifth and seventh weeks of pregnancy. At this stage, the embryo face takes on more definition as mouth perforation, nostrils and ear indentations become visible. Forensic science literature reports that ear growth after the first four months of birth is highly linear [20]. The rate of stretching is five times greater than normal during the period from 4 months to the age of 8, after which, it is constant until the age of seventy when it again increases. Thus it can be said that ear remains almost unchanged during a substantial period of 62 years and, thus, it is one of the strong points of considering ear for biometric authentication.

Haar-based methods have given fairly better results for face detection as it is robust and fast. The different types of ear recognition systems include those of intensity-based, force-field based, 2D curves geometry, wavelet transformation, Gabor filters, SIFT, and 3D features. The force-field transforms gained popularity due to its uniqueness and efficiency [22]. Similar methods have also been implemented on other kinds of ear recognition systems [8][10].

Deep Methods have already come up and showing good performances on other face recognition systems which shows that it can also be applied to ear recognition systems. Hand-crafted feature detectors have not been able to work properly and are not robust, so deep features have been extracted to improve upon the performance. But one of the few drawbacks about deep learning is that it needs a large amount of data to train the model. There are not many ear databases that are too big but an attempt has been made to apply deep learning on a small scale database and analyze the results.

The goal of this project is to build deep learning models that can be robust and can be applied to all sorts of recognition processes. In traditional methods, the result did not depend on the size of the dataset as the features are extracted individually and no layers are being formed as the network was shallow, but deep learning models need a lot of data to process and as a matter of fact, a lot of data is needed to extract the individual features from the respective images. One of the primary reasons behind Deep Learning models failing in the last decade was that there was not enough data to train the deep learning models. The availability of vast amount of data in every domain ranging from images to raw data has made it possible for deep learning systems to give better performances.

Another reason deep learning algorithms were not being able to perform properly was that of computation ability of CPUs, the advent of modern GPUs have made the training process even faster. Companies like NVIDIA and others have been in the forefront to provide GPUs for research and commercial purposes. Training time has been decreased significantly even after processing millions of images. Thus with the help of great deep learning frameworks like CAFFE[], Theano[], Torch[] , NVIDIA Digits[], Google TensorFlow[] and their combination with GPUs , training has been made faster and thus more and more methods can be applied. Various research institutions and commercial companies have been using deep learning since the last decade and getting tremendous success in object recognition, object classification, image retrieval, image classification, speech recognition, object recognition, biometrics, data analytics, machine learning and many other techniques.

It has also been seen that one deep learning model can be used for a variety of tasks ranging from working with raw data to images to speech data etc. Thus it makes it easier to create one model and use everywhere unlike the shallow techniques which had to be specifically hand-crafted for specific purposes and thus imposed a great problem for researchers. Currently, researchers are working on the disadvantages that deep learning has which is, the model needs a lot of data to be trained but humans can recognize themselves with very few training data and that is why research is going in this field to eliminate this barrier so that deep learning techniques will be able to learn from a small dataset and give equally good performances that they are producing on a model trained with a large dataset.

A lot of work has been happening in the ear biometrics over the past decade. The approaches are varied with some working on Intensity-based features while others on 2-D ad 3-D curves etc. Chang et al.[2003] whole worked on the UND database and got an accuracy of 72.7p.c. using the PCA approach. A new concept called Force-Field was being brought by Hurley et al. which gave an accuracy of 99.2 p.c. on the XM2VTS dataset. Many other approaches like 3D Features, Gabor Filters, SIFT, Wavelet Transformation have been applied on different databases and results have been obtained. This project is mostly on the analysis of Biometric Humar Ear datasets on two methods - SIFT and SURF and a comparison is given on the rotation and scaling factors and how the number of features varies on such conditions keeping the real life scenarios in mind where Ear images are not obtained as compared to a dataset.

Similar methods have been applied on other Biometics like face, fingerprint and others etc. Both Shallow and Deep methods have different significances. Shallow methods start by extracting a representation of the image using hand-crafted local image descriptors like SIFT, SURF ,LBP,HoG [5,13,22,23,32: Parkhi paper]' then the local features are being aggregated into an overall ace descriptor by using a pooling mechanism for example Fisher Vector[15,20:Parkhi paper]. The work of deep learning was initiated with the help of a CNN Feature Extractor , a learnable function obtained by composing several linear and non-linear operators. The best example can be shown in DeepFace[29:Parkhi paper] which uses deep CNN trained to classify faces using a dataset of 4 million examples spanning 4000 unique identities. Upon extensive research, new ideas were being incorporated using multiple CNNs[25:Parkhi paper].

